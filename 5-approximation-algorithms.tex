\section{Approximations-Algorithmen}

\begin{takeaway}
    \item Relativer Fehler, Approximationsgüte
    \item Beispiele: 2-Approximation Min-VCP, Spannbaum-TSP, Christofides (metrisches TSP)
    \item Nichtapproximierbarkeit
    \item Approximationsschemata (PTAS, FPTAS)
\end{takeaway}

\paragraph{Motivation}
Schwere Optimierungsprobleme: Statt Abstriche bei der Laufzeit zu machen (Parametrisierung,
exponentiell aber mit kleiner Basis), machen wir Abstriche bei der Optimalität.
Ein Approximationsalgorithmus berechnet eine zulässige Lösung (feasible solution), dessen Qualität
nicht weit von der Qualität einer optimalen Lösung abweicht.

\paragraph{Definitionen}
Sei $U = (L, \M, cost, goal)$ ein Optimierungsproblem und sei $\A$ ein Algorithmus für $U$.
{
\newcommand{\opt}{Opt_U(I)}
\newcommand{\cost}{cost(\A(I))}
\begin{itemize}
    \item Für alle $I \in L$ ist der \emph{relative Fehler von $\A$ auf $I$}
    $$ \varepsilon_\A(I) := \frac{|\cost - \opt|}{\min \{ \opt, \cost \} } $$
    \item Für alle $n \in \N$ ist der \emph{relative Fehler von $\A$}
    $$ \varepsilon_\A(n) := \max_{I \in L} \{ \varepsilon_\A(I) \st |I|=n \} $$
    \item Für alle $I \in L$ ist die \emph{Approximationsgüte (approximation ratio) von $\A$ auf $I$}
    $$ R_\A(I) := \max \left\{ \frac{\cost}{\opt}, \frac{\opt}{\cost} \right\} $$
    \item Für alle $n \in \N$ ist die \emph{Approximationsgüte (approximation ratio) von $\A$}
    $$ R_\A(n) := \max_{I \in L} \{ R_\A(I) \st |I|=n \} $$
    \item Für alle positiven reellen $\delta > 1$ heisst $\A$ \emph{$\delta$-Approximationsalgorithmus für $U$} falls gilt
    $$ \forall I \in L \; . \; R_\A(I) \leq \delta $$
    \item Für jede Funktion $f : \N \mapsto \R^+$ heisst $\A$ \emph{$f(n)$-Approximationsalgorithmus für $U$} falls gilt
    $$ \forall n \in \N \; . \; R_\A(I) \leq f(n) $$
\end{itemize}
Wobei die Definitionen sowohl $goal=\min$ als auch $goal=\max$ sinnvoll beschreiben.
}

\paragraph{Beispiel: 2-Approximationsalgorithmus für Min-VCP}
Idee: berechne ein maximales Matching $M$ und ein approximiertes vertex cover $C$.
\begin{enumerate}
    \item Initialisierung: Sei $C \leftarrow \emptyset, M \leftarrow \emptyset, E' \leftarrow E$.
    \item while $E' \neq \emptyset$ do: \\
    Wähle eine beliebige Kante $\{u,v\} \in E'$. \\
    Update: $C \leftarrow C \cup \{u,v\} , M \leftarrow M \cup \{ \{u,v\} \} ,
    E' \leftarrow E' - \{\text{all edges incident to $u$ or $v$}\} $
\end{enumerate}

\paragraph{Theorem}
Obiger Algorithmus ist ein 2-Approximationsalgorithmus für das Min-VCP.

\underline{Beweis:}
Korrektheit: nur Kanten die mindestens einen Nachbarn in $C$ haben werden entfernt, daher ist $C$ ein vertex cover. \\
Laufzeit: Linearzeit, $\bigO(|E|)$. \\
2-Approximation: $|C| = 2 \cdot |M|$.
Um $|M|$ viele Kanten in $M$ (und damit auch in $G$, da $M \subseteq E$) zu überdecken,
werden $\geq |M|$ viele Knoten benötigt. \\
$ \implies Opt_{Min-VCP}(G) \geq |M| \iff |C| \leq 2 \cdot Opt_{Min-VCP}(G) $.

\paragraph{Beispiel: Metrisches-TSP ($\Delta$-TSP)}
Eingabe: vollständiger, gewichteter Graphen $G=(V,E)$ und Kostenfunktion $c : E \mapsto \N^+$.
Metrisch == Dreiecksungleichung gilt:
$$ \forall u,v,w \in V \cl c(\{u,v\}) \leq c(\{u,w\}) + c(\{w,v\}) $$

\paragraph{Spannbaum-Algorithmus für $\Delta$-TSP}
Idee: minimum Hamiltonkreise zu berechnen ist schwer, aber minimale Spannbäume sind einfach (Polynomzeit).
\begin{enumerate}
    \item Berechne einen MST $T$ von $G$
    \item Wähle einen beliebigen $v \in V$.
    Berechne eine DFS in $T$ ausgehend von $v$ und ordne dabei die Knoten in Besuchsreihenfolge $\overline{H}$.
    \item return Hamiltonkreis $H = \overline{H}, v$
\end{enumerate}

\paragraph{Theorem}
Der Spannbaum-Algorithmus ist ein 2-Approximationsalgorithmus für das $\Delta$-TSP.

\underline{Beweis:}
Korrektheit: offensichtlich. \\
Laufzeit: polynomiell, z.B. $\bigO(|E| \log|V|)$ mit Kruskal-MST. \\
2-Approximation:
Sei $H_{Opt}$ ein optimaler Hamiltonkreis, d.h. $cost(H_{Opt}) = Opt_{\Delta-TSP}(I)$.
Dann gilt:
$$ cost(T) = \sum_{e \in E(T)} c(e) \leq cost(H_{Opt}) $$
da $H_{Opt}$ durch Entfernen einer Kante zu einem Spannbaum wird und $T$ ein minimaler Spannbaum ist.
\\
Sei $W$ die Eulertour die $\A$ auf dem Multigraphen $T_2$ läuft
(wobei $T_2$ aus $T$ durch Verdoppeln aller Kanten gebildet wird).
Die Kosten von ausgegebenen $H$ ist eine E
$$ cost(W) = 2 \cdot cost(T) \leq 2 \cdot cost(H_{Opt}) $$
Wenn in $H$ noch doppelt vorkommende Knoten abgekürzt werden, gilt dank der Dreiecksungleichung:
$$ cost(H) \leq cost(W) $$
Insgesamt gilt also $ cost(H) \leq 2 \cdot cost(H_{Opt})$, also ist $\A$ eine 2-Approximation.

\paragraph{Lemma}
In jedem Graphen $T$ ist die Anzahl Knoten mit ungeradem Grad gerade.

\underline{Beweis:}
Summe $t$ aller Grade ist $2 \cdot |E|$ (gerade), Summe $e$ aller geraden Grade ist gerade, also ist
die Differenz $o=t-e$ auch gerade.
Da alle Summanden in der Summe $o$ aller ungeraden Grade ungerade sind muss die Anzahl Summanden
(und damit die Anzahl Knoten mit ungeradem Grad) gerade sein.

\paragraph{Christofides-Algorithmus für $\Delta$-TSP}
Idee: für eine Eulertour braucht jeder Knoten geraden Grad.
Berechne ein perfektes Matching auf den Knoten mit ungeradem Grad und vereinige es mit dem MST.
\begin{enumerate}
    \item Berechne eine MST $T$ von $G$
    \item Sei $S \leftarrow \{ v \in V \st \deg_T(v) \text{ ungerade} \}$
    \item Berechne ein perfektes Matching $M$ auf $S$ in $G$ mit minimalen Kosten
    \item Berechne eine Eulertour $\omega$ auf dem Multigraphen $G' = (V, E(T) \cup M)$
    \item Verkürze $\omega$ zu einem Hamiltonkreis $H$ in $G$ und gebe $H$ aus
\end{enumerate}

\paragraph{Theorem}
Der Christofides-Algorithmus ist ein 1.5-Approximationsalgorithmus für das $\Delta$-TSP.

\underline{Beweis:}
Korrektheit: offensichtlich. \\
Laufzeit: nicht-trivial (insbesondere perfektes Matching), aber in Polynomzeit $\bigO(n^4)$. \\
1.5 Approximation:
Es gilt ($\star$ wegen der Dreiecksungleichung):
$$ cost(H) \overset{\star}{\leq} cost(\omega) = cost(G') = \sum_{e \in E(T) \cup M} c(e) = cost(T) + cost(M) $$
Und wie beim Spannbaum-Algorithmus gilt $cost(T) \leq cost(H_{Opt})$.

Zeige nun dass $cost(M) \leq \frac{1}{2} \cdot cost(H_{Opt})$ gilt:
\\
Sei $S=\{ v_1, \dots , v_{2m} \}$ die Menge von Knoten mit ungeradem Grad in $T$, o.B.d.A. so dass sie
in dieser Reihenfolge in $H_{Opt}$ vorkommen:
$$ H_{Opt} = v_1, \alpha_1, v_2, \alpha_2, \dots, \alpha_{2m-1}, v_{2m}, \alpha_{2m}, v_1 $$
wobei $\alpha_i$ die (womöglich leeren) Pfade zwischen den $v_i$ in $H_{Opt}$ sind.
\\
Betrachte die disjunkten Matching $M_1, M_2$:
\begin{align*}
M_1 & := \{ \{v_1,v_2\}, \{v_3,v_4\}, \dots, \{v_{2m-1},v_{2m}\} \} \\
M_2 & := \{ \{v_2,v_3\}, \{v_4,v_5\}, \dots, \{v_{2m-2},v_{2m-1}\}, \{v_{2m},v_1\} \}
\end{align*}
Dann gilt ($\star$ wegen der Dreiecksungleichung und den übersprungenen $\alpha_i$):
$$ cost(M_1) + cost(M_2) = \sum_{i=1}^{2m} cost(v_i, v_{(i+1) \mod 2m}) \overset{\star}{\leq} cost(H_{Opt}) $$
Ausserdem gilt $cost(M) \leq cost(M_1), cost(M_2)$ da $M_1, M_2$ ebenfalls perfekte Matchings in $S$ sind
(aber $M$ ein min-cost perfektes Matching ist).
Es folgt:
$$ cost(M) \leq \min \{ cost(M_1), cost(M_2) \} \overset{\star}{\leq} \frac{1}{2} \cdot cost(H_{Opt}) $$
wobei $\star$ gilt weil nicht beide grösser als  $\frac{1}{2} \cdot cost(H_{Opt})$ sein können.


\subsection{Nichtapproximierbarkeit}

\paragraph{Theorem}
Es existiert kein (polynomieller) $2^n$-Approximationsalgorithmus für das allgemeine TSP, es sei denn P=NP.

\underline{Beweis:}
Reduktion von einem NP-schweren (Entscheidungs-)Problem (z.B. Hamiltonkreisproblem) auf das Problem eine
approximierte TSP-Lösung zu finden.

Reduktionsidee: Füge fehlende Kanten hinzu, so dass $G'$ vollständig ist.
Definiere Kantengewichte:
$$ c(e) = \begin{cases}
1 & \text{ if } e \in E \\
n \cdot 2^n + 1 & \text{ if } e \notin E
\end{cases} $$
Sei $\A$ ein Approximationsalgorithmus der auf Eingabe $(G', c)$ eine $2^n$-Approximation berechnet.
\\
Fallunterscheidung:
\begin{enumerate}
    \item $\exists \text{ Hamiltonkreis in } G \implies \exists \text{ Hamiltonkreis in } G' $ mit Kosten $n$
    \item $\nexists \text{ Hamiltonkreis in } G \implies \forall \text{ Hamiltonkreise in } G' $ haben Kosten
    $\geq (n-1) + (n \cdot 2^n + 1) > 2^n \cdot n $
\end{enumerate}
Ist die Ausgabe von $\A$ nun $> 2^n \cdot n$ dann weiss man, dass kein Hamiltonkreis der Kosten $n$ in $G$ existiert,
da $\A$ ein $2^n$-Approximationsalgorithmus ist.
Also kann $\A$ genutzt werden um das Hamiltonkreisproblem zu lösen, was ein Widerspruch zu P $\neq$ NP ist.


\subsection{Approximationsschemata}

\paragraph{Definition: PTAS/FPTAS}
{
\newcommand{\eps}{\varepsilon^{-1}}
Sei $U = (L, \M, cost, goal)$ ein Optimierungsproblem und sei $\A$ ein konsistenter Algorithmus für $U$.

$\A$ heisst \emph{polynomielles Approximationsschema (polynomial-time approximation scheme PTAS)}
für $U$, falls gilt: \\
für alle $(I,\varepsilon) \in L \times \R^+$ berechnet $\A$ eine zulässige Lösung $\A(I)$ mit
$R_\A(I) \leq 1+\varepsilon$ und die Laufzeit $\Time_\A(I, \eps)$ ist polynomiell in $|I|$.%
\footnote{Aber ohne Aussage über die Abhängigkeit von $\eps$, d.h. insbesondere darf $\Time_\A(I, \eps)$
exponentiell in $\eps$ sein.}

$\A$ heisst \emph{echtes polynomielles Approximationsschema (fully polynomial-time approximation scheme FPTAS)}
für $U$, falls zusätzlich gilt: \\
die Laufzeit $\Time_\A(I, \eps)$ ist polynomiell in $|I|$ \underline{und} $\eps$.
}

\paragraph{Einfaches Rucksackproblem (Simple Knapsack Problem SKP)} \mbox{} \\
Eingabe: $w_1, \dots, w_n, b \in \N^+$ (daher ``einfach'': Gewicht = Kosten) \\
Zulässige Lösungen: $T \subseteq \{1, \dots, n\}$ so dass $\sum_{i \in T} w_i \leq b$ \\
Kosten: $cost(T) = \sum_{i \in T} w_i$ \\
Ziel: $\max$

\paragraph{Algorithmus: Greedy-SKP}
\begin{enumerate}
    \item Sortiere absteigend: $b \geq w_1 \geq \dots \geq w_n$
    \item Initialisierung: $T \leftarrow \emptyset; cost(T) \leftarrow 0$
    \item for all $i \in \{1, \dots, n\}$ do:
    if $cost(T)+w_i \leq b$ then $T \leftarrow T \cup \{i\}; cost(T) \leftarrow cost(T)+w_i$
\end{enumerate}

\paragraph{Theorem}
Greedy-SKP ist eine 2-Approximation für das SKP.

\underline{Beweis:}
Laufzeit: $\bigO(n \log n)$ \\
2-Approximation:
Zeige: $T$ ist optimal oder $cost(T) \geq \frac{b}{2}$.
Falls $T=\{1, \dots, n\}$ dann ist $T$ optimal.
Andernfalls sei $j$ der kleinste Index mit $j \notin T$ (da $b \geq w_1$ gilt $j \geq 2$).
Dann gilt $cost(T) > b -w_j$ und $cost(T) \geq w_1 \geq w_j$.
Addieren und durch 2 teilen ergibt: $cost(T) \geq (b-w_j+w_j)/2 = b/2$.

\paragraph{Algorithmus: PTAS-SKP}
Idee: Steuere durch den Parameter $\varepsilon$ mit wie viel Aufwand eine optimale Lösung ausprobiert werden
soll, und mache den Rest greedy. \\
Eingabe: $w_1, \dots, w_n, b \in \N^+$, $\varepsilon \in (0,1)$ (für $\varepsilon \geq 1$ nehme direkt Greedy-SKP)
\begin{enumerate}
    \item Sortiere absteigend: $b \geq w_1 \geq \dots \geq w_n$
    \item $k \leftarrow \lceil \frac{1}{\varepsilon} \rceil$
    \item Für alle $T \subseteq \{1, \dots, n\}$ mit $|T| \leq k$ und $\sum_{i \in T} w_i \leq b$:
    erweitere $T$ zu $T^*$ mit Greedy-SKP
    \item return das $T^*$ mit maximalem Gewicht
\end{enumerate}
Achtung: Wir betrachten alle Teilmengen der Grösse $k$, \underline{nicht} alle Teilmengen der ersten $k$ Elemente!

\paragraph{Theorem}
PTAS-SKP ist ein PTAS für das SKP.

\underline{Beweis:}
Laufzeit: $\bigO(n \log n + 1 + n^k \cdot n) = \bigO(n^{\lceil \frac{1}{\varepsilon} \rceil + 1})$
$\rightarrow$ polynomiell für fixes $\varepsilon$, aber exponentiell in $\varepsilon$

$(1+\varepsilon)$-Approximation: \\
Sei $T_{Opt} = \{i_1, \dots, i_p\}$ eine optimale Lösung für Eingabe $I$, d.h. $cost(T_{Opt})=Opt_{SKP}(I)$.
Fallunterscheidung:
\begin{itemize}
    \item[$p \leq k$:] PTAS-SKP betrachtet die Menge $T_{Opt}$, also ist $T^*$ optimal und der relative Fehler ist 0.
    \item[$p > k$:] PTAS-SKP betrachtet die Menge $P=\{i_1, \dots, i_k\}$, d.h. die $k$ schwersten Elemente
    der optimalen Lösung.
    Sei $P^*$ die berechnete greedy Erweiterung von $P$. Falls $P^* = T_{Opt}$ sind wir fertig.
    Sonst existiert ein $i_q \in T_{Opt}-P^*$ mit $i_q > i_k \geq k$. Es gilt:
    $$ cost(P^*) + w_{i_q} > b \geq cost(T_{Opt}) $$
    Dann gilt $\star$ mit einem Durchschnitts-Argument (da $w_{i_q}$ das kleinste Element der Summe ist):
    $$ w_{i_q} \overset{\star}{\leq} \frac{ w_{i_1} + \dots + w_{i_k} + w_{i_q} }{k+1} \leq \frac{cost(T_{Opt})}{k+1} $$
    Umformen ergibt:
    \begin{align*}
    R_{PTAS-SKP}(I)(I, \varepsilon) & \overset{\text{Def}}{=} \frac{cost(T_{Opt})}{cost(T^*)}
    \leq \frac{cost(T_{Opt})}{cost(P^*)} \\
    & \leq \frac{cost(T_{Opt})}{cost(T_{Opt}) - w_{i_q}}
    \leq \frac{cost(T_{Opt})}{cost(T_{Opt}) - \frac{cost(T_{Opt})}{k+1} } \\
    & = \frac{1}{1 - \frac{1}{k+1} } = \frac{k+1}{k} = 1 + \frac{1}{k} \leq 1 + \varepsilon
    \end{align*}
\end{itemize}

\paragraph{Algorithmus: FPTAS-KP}
Idee: Ähnlich zum pseudopolynomiellem DP-Algorithmus (siehe Seite \pageref{algo-kp}).
Kostenwerte treiben Laufzeit nach oben, also runde sie.
\\
Eingabe: $w_1, \dots, w_n, c_1, \dots, c_n, b \in \N^+$, $\varepsilon \in (0,1)$
\begin{enumerate}
    \item Initialisierung: $c_{max} \leftarrow \max \{c_1, \dots, c_n\}$;
    Rundungsfaktor $t \leftarrow \frac{\varepsilon \cdot c_{max}}{(1+\varepsilon) \cdot n}$
    \item for all $i \in \{1, \dots, n\}$ do: $c_i' \leftarrow \lfloor \frac{c_i}{t} \rfloor$
    \item Berechne eine optimale Lösung $T'$ auf Eingabe $I' = w_1, \dots, w_n, c_1', \dots, c_n', b$ mit Dyn-Prog-KP
    \item return $T'$
\end{enumerate}

\paragraph{Theorem}
FPTAS-KP ist ein FPTAS für das KP.

\underline{Beweis:}
Korrektheit: Gewichte bleiben unverändert, daher ist $T'$ eine zulässige Lösung für $I$.

Approximationsgüte:
Voller Beweis siehe Buch Kapitel 7.5, Seite 217f.
Umformen ergibt:
$$ cost(T_{Opt}, I) \geq  cost(T', I) \geq cost(T_{Opt}, I) - n \cdot t $$
$$
\implies 0 \leq cost(T_{Opt}, I) - cost(T', I) \leq n \cdot t
\overset{\text{Def}}{\leq} n \cdot \frac{\varepsilon \cdot c_{max}}{(1+\varepsilon) \cdot n}
\leq \frac{\varepsilon}{(1+\varepsilon)} \cdot c_{max}
$$
D.h. die Differenz zwischen der optimalen und der berechneten Lösung ist beschränkt durch eine Funktion
in $\varepsilon$ und $c_{max}$.
\\
Weiteres Umformen (siehe Buch) ergibt wie gewünscht: $R_{FPTAS-KP}(I, \varepsilon) \leq 1 + \varepsilon$.

Laufzeit: Es gilt:
$$ \bigO(n + n + n \cdot \sum_{j=1}^n c_j') = \bigO(\varepsilon^{-1} \cdot n^3) $$
dank folgender Abschätzung:
\begin{align*}
\sum_{j=1}^n c_j'
= \sum_{j=1}^n \left\lfloor \frac{c_j}{t} \right\rfloor
\leq \sum_{j=1}^n \left( c_j \cdot \frac{(1+\varepsilon) \cdot n}{\varepsilon \cdot c_{max}} \right)
= \frac{1+\varepsilon}{\varepsilon} \cdot \frac{n}{c_{max}} \cdot \sum_{j=1}^n c_j
\leq (1+\varepsilon) \cdot \varepsilon^{-1} \cdot n^2
\in \bigO(\varepsilon^{-1} \cdot n^2)
\end{align*}
Die Laufzeit ist polynomiell in $n$ und $\varepsilon^{-1}$, also ist der Algorithmus ein FPTAS für das KP.
